{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b26da3d",
   "metadata": {},
   "source": [
    "# Activity 5.3 - Returning to the Nest\n",
    "\n",
    "## Processing Dr. Bergen's Eagle Data in `pyspark`\n",
    "\n",
    "In a previous homework, you performed a data management task for Dr. Bergen, Director of the WSU Statistical Consulting Center.  The associated data can be found in the `data` folder of this repository.  Below, you will find the instructions for the original task.\n",
    "\n",
    ">    Dr. Bergen had the following to say about the data.\n",
    ">\n",
    ">     - One row = one GPS measurement.  \n",
    ">     - Subsample of 10K GPS points from a couple bald eagles in Iowa. \n",
    ">     - **Context.** need to use the flight characteristics to perform $k$-means clustering of the flight points.  \n",
    ">\n",
    ">    Variables to be used for clustering include\n",
    ">\n",
    ">    - `KPH` (km per hour; an instantaneous measure of speed; measured by the GPS device);\n",
    ">    - `Sn` (an average speed; given 2 time points and at locations and something like );\n",
    ">    - `AGL0` (meters above ground level);\n",
    ">    - `VerticalRate` (change in AGL between two time points; large negative if descending quickly; large positiveif ascending quickly);\n",
    ">    - `absVR` (absolute value of VerticalRate); and\n",
    ">    - `abs_angle`c(absolute value of turn angle, in radians; larger values equal more “tortuous”, i.e. twisty flight)\n",
    ">\n",
    ">    All variables except for `VerticalRate` are skewed and all variables need to be mean-centered and standardized prior to clustering.\n",
    ">\n",
    ">    <img src=\"./img/summary_of_features.png\"/>\n",
    ">\n",
    ">    Note that data is \n",
    ">\n",
    ">    - *mean-centered* by subtracting the mean of the column from each entry.\n",
    ">    - *standardized* by dividing each entry by the standard deviation of the column.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "In this activity, you will redo the following tasks in `pyspark` using the STACK-TRANSFORM-UNSTACK trick.\n",
    "\n",
    "- Read the data into `pyspark` and assure that the columns have the correct type.  Define a schema as needed.\n",
    "- Apply `sqrt` transform to `KPH`, `Sn`, `AGL0`, `absVR` and `abs_angle`.  \n",
    "- Mean-center and standardize transformed variables from above as well as `VerticalRate`\n",
    "- Visualize the transformed features.  \n",
    "    - Because `pyspark` lacks visualization tools, you should convert the results back to a `pandas.Dataframe` then use a [seaborn multi-plot grid](https://seaborn.pydata.org/tutorial/axis_grids.html) to plot all the variables on the same panel.  **HINT.** To make this work, you will need to stack all of the transformed features.\n",
    "\n",
    "**Deliverables.** You should keep any code cells you used to test/figure-out the solution, but the end result should be two cells,\n",
    "\n",
    "1. A cell loading spark and reading in the data frame.\n",
    "2. A second cell containing all the code and data management in one dot chain; along with any other objects used in the pipe.\n",
    "3. A third cell containing all the code needed to convert the data frame back to pandas and create your visualization.\n",
    "\n",
    "Note that these three cells should work independent of the rest of your code: If I restart the kernel and run only these cells, everything should work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3fd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint 1.  pyspark includes sqrt, mean, and sd functions.\n",
    "from pyspark.sql.functions import sqrt, mean, stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efa3ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /Users/toddiverson/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (9.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /Users/toddiverson/.pyenv/versions/anaconda3-2022.05/lib/python3.9/site-packages (from pyarrow) (1.21.5)\r\n"
     ]
    }
   ],
   "source": [
    "# Hint 2.  The Apache Arrow library allows fast conversion of data frames back to pandas.  \n",
    "!pip install pyarrow\n",
    "\n",
    "# The `toPandas` method effectively replaces `collect`. \n",
    "# Example:\n",
    "# pandas_df = spark_df.toPandas() # <== requires pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b19cefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
